{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1JDDKsrYkC6FGLRWI0ZjaCDDsequtpAez","authorship_tag":"ABX9TyOxv4VNP+RrhJHFL2E3lAwC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m3-R31V5TdiV","executionInfo":{"status":"ok","timestamp":1705032158399,"user_tz":-330,"elapsed":14310,"user":{"displayName":"Shivam Thakur","userId":"14758005208091355129"}},"outputId":"464549d0-b975-4892-d246-5398c8ccab27"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["\n","#!pip install -q transformers\n","!pip install -q easyocr\n","\n","from transformers import AutoModelForObjectDetection\n","from transformers import TableTransformerForObjectDetection\n","import torch\n","import os\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from matplotlib.patches import Patch\n","import numpy as np\n","import csv\n","import easyocr\n","from tqdm.auto import tqdm\n","import csv"],"metadata":{"id":"0p2vDxSmUrTx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# class to prepare image\n","class MaxResize(object):\n","    def __init__(self, max_size=800):\n","        self.max_size = max_size\n","\n","    def __call__(self, image):\n","        width, height = image.size\n","        current_max_size = max(width, height)\n","        scale = self.max_size / current_max_size\n","        resized_image = image.resize((int(round(scale*width)), int(round(scale*height))))\n","\n","        return resized_image"],"metadata":{"id":"qx0S3ZNtUrQM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_image_urls(directory_path):\n","    image_urls = []\n","\n","    # Iterate through all files in the directory\n","    for filename in os.listdir(directory_path):\n","        # Check if the file is an image (you can customize the list of valid extensions)\n","        valid_extensions = ['.jpg', '.jpeg', '.png']\n","        for i in valid_extensions:\n","          if filename.lower().endswith(i):\n","            # Create the URL by joining the directory path and filename\n","            image_url = os.path.join(directory_path, filename)\n","            # Append the URL to the list\n","            image_urls.append(image_url)\n","\n","    return image_urls"],"metadata":{"id":"tKeo53VOUrN6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for output bounding box post-processing\n","def box_cxcywh_to_xyxy(x):\n","    x_c, y_c, w, h = x.unbind(-1)\n","    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n","    return torch.stack(b, dim=1)\n","\n","\n","def rescale_bboxes(out_bbox, size):\n","    img_w, img_h = size\n","    b = box_cxcywh_to_xyxy(out_bbox)\n","    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n","    return b\n"],"metadata":{"id":"Og9tboXzUrLY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def outputs_to_objects(outputs, img_size, id2label):\n","    m = outputs.logits.softmax(-1).max(-1)\n","    pred_labels = list(m.indices.detach().cpu().numpy())[0]\n","    pred_scores = list(m.values.detach().cpu().numpy())[0]\n","    pred_bboxes = outputs['pred_boxes'].detach().cpu()[0]\n","    pred_bboxes = [elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)]\n","\n","    objects = []\n","    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):\n","        class_label = id2label[int(label)]\n","        if not class_label == 'no object':\n","            objects.append({'label': class_label, 'score': float(score),\n","                            'bbox': [float(elem) for elem in bbox]})\n","\n","    return objects"],"metadata":{"id":"5oDbeIMxUrI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fig2img(fig):\n","    \"\"\"Convert a Matplotlib figure to a PIL Image and return it\"\"\"\n","    import io\n","    buf = io.BytesIO()\n","    fig.savefig(buf)\n","    buf.seek(0)\n","    img = Image.open(buf)\n","    return img"],"metadata":{"id":"CEcLFSbnUq52"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_detected_tables(img, det_tables, out_path=None):\n","    plt.imshow(img, interpolation=\"lanczos\")\n","    fig = plt.gcf()\n","    fig.set_size_inches(20, 20)\n","    ax = plt.gca()\n","\n","    for det_table in det_tables:\n","        bbox = det_table['bbox']\n","\n","        if det_table['label'] == 'table':\n","            facecolor = (1, 0, 0.45)\n","            edgecolor = (1, 0, 0.45)\n","            alpha = 0.3\n","            linewidth = 2\n","            hatch='//////'\n","        elif det_table['label'] == 'table rotated':\n","            facecolor = (0.95, 0.6, 0.1)\n","            edgecolor = (0.95, 0.6, 0.1)\n","            alpha = 0.3\n","            linewidth = 2\n","            hatch='//////'\n","        else:\n","            continue\n","\n","        rect = patches.Rectangle(bbox[:2], bbox[2]-bbox[0], bbox[3]-bbox[1], linewidth=linewidth,\n","                                    edgecolor='none',facecolor=facecolor, alpha=0.1)\n","        ax.add_patch(rect)\n","        rect = patches.Rectangle(bbox[:2], bbox[2]-bbox[0], bbox[3]-bbox[1], linewidth=linewidth,\n","                                    edgecolor=edgecolor,facecolor='none',linestyle='-', alpha=alpha)\n","        ax.add_patch(rect)\n","        rect = patches.Rectangle(bbox[:2], bbox[2]-bbox[0], bbox[3]-bbox[1], linewidth=0,\n","                                    edgecolor=edgecolor,facecolor='none',linestyle='-', hatch=hatch, alpha=0.2)\n","        ax.add_patch(rect)\n","\n","    plt.xticks([], [])\n","    plt.yticks([], [])\n","\n","    legend_elements = [Patch(facecolor=(1, 0, 0.45), edgecolor=(1, 0, 0.45),\n","                                label='Table', hatch='//////', alpha=0.3),\n","                        Patch(facecolor=(0.95, 0.6, 0.1), edgecolor=(0.95, 0.6, 0.1),\n","                                label='Table (rotated)', hatch='//////', alpha=0.3)]\n","    plt.legend(handles=legend_elements, bbox_to_anchor=(0.5, -0.02), loc='upper center', borderaxespad=0,\n","                    fontsize=10, ncol=2)\n","    plt.gcf().set_size_inches(10, 10)\n","    plt.axis('off')\n","\n","    if out_path is not None:\n","      plt.savefig(out_path, bbox_inches='tight', dpi=150)\n","\n","    return fig"],"metadata":{"id":"mwSnPLHmUqvQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def objects_to_crops(img, tokens, objects, class_thresholds, padding=10):\n","    \"\"\"\n","    Process the bounding boxes produced by the table detection model into\n","    cropped table images and cropped tokens.\n","    \"\"\"\n","\n","    table_crops = []\n","    for obj in objects:\n","        if obj['score'] < class_thresholds[obj['label']]:\n","            continue\n","\n","        cropped_table = {}\n","\n","        bbox = obj['bbox']\n","        bbox = [bbox[0]-padding, bbox[1]-padding, bbox[2]+padding, bbox[3]+padding]\n","\n","        cropped_img = img.crop(bbox)\n","\n","        table_tokens = [token for token in tokens if iob(token['bbox'], bbox) >= 0.5]\n","        for token in table_tokens:\n","            token['bbox'] = [token['bbox'][0]-bbox[0],\n","                             token['bbox'][1]-bbox[1],\n","                             token['bbox'][2]-bbox[0],\n","                             token['bbox'][3]-bbox[1]]\n","\n","        # If table is predicted to be rotated, rotate cropped image and tokens/words:\n","        if obj['label'] == 'table rotated':\n","            cropped_img = cropped_img.rotate(270, expand=True)\n","            for token in table_tokens:\n","                bbox = token['bbox']\n","                bbox = [cropped_img.size[0]-bbox[3]-1,\n","                        bbox[0],\n","                        cropped_img.size[0]-bbox[1]-1,\n","                        bbox[2]]\n","                token['bbox'] = bbox\n","\n","        cropped_table['image'] = cropped_img\n","        cropped_table['tokens'] = table_tokens\n","\n","        table_crops.append(cropped_table)\n","\n","    return table_crops"],"metadata":{"id":"kEeGYz9aVHvB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_cell_coordinates_by_row(table_data):\n","    # Extract rows and columns\n","    rows = [entry for entry in table_data if entry['label'] == 'table row']\n","    columns = [entry for entry in table_data if entry['label'] == 'table column']\n","\n","    # Sort rows and columns by their Y and X coordinates, respectively\n","    rows.sort(key=lambda x: x['bbox'][1])\n","    columns.sort(key=lambda x: x['bbox'][0])\n","\n","    # Function to find cell coordinates\n","    def find_cell_coordinates(row, column):\n","        cell_bbox = [column['bbox'][0], row['bbox'][1], column['bbox'][2], row['bbox'][3]]\n","        return cell_bbox\n","\n","    # Generate cell coordinates and count cells in each row\n","    cell_coordinates = []\n","\n","    for row in rows:\n","        row_cells = []\n","        for column in columns:\n","            cell_bbox = find_cell_coordinates(row, column)\n","            row_cells.append({'column': column['bbox'], 'cell': cell_bbox})\n","\n","        # Sort cells in the row by X coordinate\n","        row_cells.sort(key=lambda x: x['column'][0])\n","\n","        # Append row information to cell_coordinates\n","        cell_coordinates.append({'row': row['bbox'], 'cells': row_cells, 'cell_count': len(row_cells)})\n","\n","    # Sort rows from top to bottom\n","    cell_coordinates.sort(key=lambda x: x['row'][1])\n","\n","    return cell_coordinates"],"metadata":{"id":"0arhD-SwVHro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def apply_ocr(cell_coordinates):\n","    # let's OCR row by row\n","    data = dict()\n","    max_num_columns = 0\n","    for idx, row in enumerate(tqdm(cell_coordinates)):\n","      row_text = []\n","      for cell in row[\"cells\"]:\n","        # crop cell out of image\n","        cell_image = np.array(cropped_table.crop(cell[\"cell\"]))\n","        # apply OCR\n","        result = reader.readtext(np.array(cell_image))\n","        if len(result) > 0:\n","          # print([x[1] for x in list(result)])\n","          text = \" \".join([x[1] for x in result])\n","          row_text.append(text)\n","\n","      if len(row_text) > max_num_columns:\n","          max_num_columns = len(row_text)\n","\n","      data[idx] = row_text\n","\n","    print(\"Max number of columns:\", max_num_columns)\n","\n","    # pad rows which don't have max_num_columns elements\n","    # to make sure all rows have the same number of columns\n","    for row, row_data in data.copy().items():\n","        if len(row_data) != max_num_columns:\n","          row_data = row_data + [\"\" for _ in range(max_num_columns - len(row_data))]\n","        data[row] = row_data\n","\n","    return data"],"metadata":{"id":"A2RojSBYVHpZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_as_csv(file_name,data):\n","    with open(file_name,'w') as result_file:\n","        wr = csv.writer(result_file, dialect='excel')\n","        for row, row_text in data.items():\n","            wr.writerow(row_text)"],"metadata":{"id":"m9rOzH-AVHnW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMo3h4G4TWHm"},"outputs":[],"source":["if __name__==\"__main__\":\n","    # load model\n","    model = AutoModelForObjectDetection.from_pretrained(\"microsoft/table-transformer-detection\", revision=\"no_timm\")\n","    print(model.config.id2label)\n","\n","    # set to gpu\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    model.to(device)\n","    print(\"\")\n","\n","    # update id2label to include \"no object\"\n","    id2label = model.config.id2label\n","    id2label[len(model.config.id2label)] = \"no object\"\n","\n","    # extract all images from directory\n","    dir_path=r\"/content/drive/MyDrive/PAGES WITH TABLES (4)/PAGES WITH TABLES\"\n","    all_images_in_dir=get_image_urls(dir_path)\n","\n","\n","    # do detection on each image\n","    for file_path in all_images_in_dir:\n","\n","        image = Image.open(file_path).convert(\"RGB\")\n","\n","        # prepare image\n","        detection_transform=transforms.Compose([\n","            MaxResize(800),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","        pixel_values = detection_transform(image).unsqueeze(0)\n","        pixel_values = pixel_values.to(device)\n","        print(f\"Pixel value for image : {os.path.basename(file_path)} is ==> {pixel_values.shape}\")\n","\n","\n","        # froward pass\n","        with torch.no_grad():\n","            outputs = model(pixel_values)\n","\n","        # postProcessing : Next, we take the prediction that has an actual class (i.e. not \"no object\").\n","        objects = outputs_to_objects(outputs, image.size, id2label)\n","        print(f\"Objects for image : {os.path.basename(file_path)} is ==> {objects}\")\n","\n","        # to visualize image with detection run this in collab for each images\n","        #fig = visualize_detected_tables(image, objects)\n","        #visualized_image = fig2img(fig)\n","\n","\n","        \"\"\"Process the bounding boxes produced by the table detection model into\n","        cropped table images and cropped tokens.\"\"\"\n","\n","        tokens = []\n","        detection_class_thresholds = {\n","                \"table\": 0.5,\n","                    \"table rotated\": 0.5,\n","                        \"no object\": 10\n","        }\n","        crop_padding = 10\n","\n","        tables_crops = objects_to_crops(image, tokens, objects, detection_class_thresholds, padding=0)\n","        cropped_table = tables_crops[0]['image'].convert(\"RGB\")\n","        #you can visualize the crop table ==> cropped_table\n","        # save cropped table ==> cropped_table.save(f\"{image}_table.jpg\")\n","\n","\n","\n","        # load structure recognition model\n","        #new v1.1 checkpoints require no timm anymore\n","        structure_model = TableTransformerForObjectDetection.from_pretrained(\"microsoft/table-structure-recognition-v1.1-all\")\n","        structure_model.to(device)\n","\n","         # update id2label to include \"no object\"\n","        structure_id2label = structure_model.config.id2label\n","        structure_id2label[len(structure_id2label)] = \"no object\"\n","        cells = outputs_to_objects(outputs, cropped_table.size, structure_id2label)\n","        #print(cells)\n","\n","\n","        structure_transform=transforms.Compose([\n","            MaxResize(1000),\n","            transforms.ToTensor(),\n","            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        ])\n","        pixel_values = structure_transform(cropped_table).unsqueeze(0)\n","        pixel_values = pixel_values.to(device)\n","        print(f\"structure_transform Pixel value for image : {image} is ==> {pixel_values.shape}\")\n","\n","        # forward pass\n","        with torch.no_grad():\n","            outputs = structure_model(pixel_values)\n","\n","\n","        # SKIIPING FINALIZED TABLE VISUALIZATION PART.\n","\n","\n","        # apply ocr row by row\n","        cell_coordinates = get_cell_coordinates_by_row(cells)\n","        # you can see row wise data of coordinates\n","        #for row in cell_coordinates:\n","            #print(row[\"cells\"])\n","\n","        reader = easyocr.Reader(['en'])\n","        data = apply_ocr(cell_coordinates)\n","        #for row, row_data in data.items():\n","           # print(row_data)\n","\n","\n","        save_as_csv(f\"{os.path.basename(file_path)}_data.csv\",data)"]},{"cell_type":"code","source":[],"metadata":{"id":"eu7_4hqEUFf7"},"execution_count":null,"outputs":[]}]}